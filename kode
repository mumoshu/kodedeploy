#!/usr/bin/env variant

tasks:
  bucket:
    script: |
      cd deploy/terraform
      terraform output bucket

  plan:
    parameters:
    - name: application
      type: string
      description: "the target codedeploy application"
    - name: environment
      type: string
    script: |
      cd deploy/terraform
      terraform init
      terraform plan -var env={{ get "environment" }}

  import:
    description: |
      Examples:
        Import a CodeDeploy application

        ./kode import --resource aws_codedeploy_app.main --id lb-app-1

        Import a CodeDeploy deployment group

        ./kode import --resource aws_codedeploy_deployment_group.main --id lb-app-1:lb-app-1-dg

        Import a CodeDeploy service role

        ./kode import --resource aws_iam_role.codedeploy_service --id codedeploy-service

        Import a CodeDeploy service role policy attachment

        ./kode import --resource aws_iam_role_policy_attachment.codedeploy_service --id codedeploy-service/arn:aws:iam::aws:policy/service-role/AWSCodeDeployRole
    parameters:
    - name: application
      type: string
      description: "the target codedeploy application"
    - name: resource
      type: string
    - name: id
      type: string
    script: |
      cd deploy/terraform
      terraform import {{ get "resource" }} {{ get "id" }}

  apply:
    parameters:
    - name: application
      type: string
      description: "the target codedeploy application"
    - name: environment
      type: string
    script: |
      cd deploy/terraform
      terraform apply -var env={{ get "environment" }} -auto-approve

  test:
    inputs:
    - name: bucket
    script: |
      echo {{ get "bucket" }}

  release:
    tasks:
      cluster:
        parameters:
        - name: application
          type: string
          description: "the target codedeploy application"
        - name: cluster
          type: string
          description: "the target cluster to start receiving traffic"
        - name: nodegroup
          type: string
          description: "the target nodegroup to start receiving traffic"
          default: ""
        - name: bucket
          type: string
        script: |
          set -vx
          c="{{ get "cluster" }}"
          cluster=$(eksctl get cluster | awk '{print $1}' | grep $c)
          nodegroup="{{ get "nodegroup" }}"
          if [ "$nodegroup" == "" ]; then
            nodegroup=$(eksctl get nodegroup --cluster $cluster | grep $cluster | grep ami | awk '{print $2}')
          fi
          nodegroup_stack=eksctl-${cluster}-nodegroup-${nodegroup}
          nodegroup_asg_name=$(aws cloudformation describe-stack-resources --stack-name $nodegroup_stack | jq -r '.StackResources[] | select(.ResourceType == "AWS::AutoScaling::AutoScalingGroup") | .PhysicalResourceId')

          app="{{ get "application" }}"
          asg=${nodegroup_asg_name}
          key=codedeploy/node/myrev.zip
          bucket="{{ get "bucket" }}"

          aws deploy push --application-name $app --description "test deployment" --ignore-hidden-files --s3-location s3://${bucket}/${key} --source deploy/node
          aws deploy create-deployment --application-name $app --deployment-group-name ${app}-dg --s3-location bucket=${bucket},key=${key},bundleType=zip --target-instances "{\"autoScalingGroups\":[\"${asg}\"]}"

  logs:
    parameters:
    - name: application
      type: string
      default: ""
    - name: namespace
      type: string
      default: ""
    - name: environment
      type: string
    - name: cluster
      type: string
      description: "the name of the cluster to deploy the thing. basically use for one-off jobs"
      default: ""
    script: |
      app={{ get "application" }}
      env={{ get "environment" }}
      ns={{ get "namespace" }}

      if [ "${app}" == "" ]; then
        app=${ns}
      fi

      codedeploy="kodedeploy-env-${env}-ns-${ns}-app-${app}"

      env=${env?missing value. specify e.g. production, staging, test, preview}
      ns=${ns?missing value. specify k8s namespace = your team, product, or project name}
      group="kodedeploy-env-${env}-ns-${ns}"
      if [ "${cluster}" != "" ]; then
        group="${group}-cluster-${cluster}"
      fi

      deploy_group_id=$(aws deploy get-deployment-group --deployment-group-name $group --application $app | tee /dev/stderr | jq .DeploymentGroup.Id)
      deploy_id=$(aws deploy get-deployment --deployment-group-name $group --application $app | tee /dev/stderr | jq .Deployment.Id)

      cw tail --stream-name --follow kodedeploy/deploys/"${group}" "${deploy_id}"/\*

  init:
    parameters:
    - name: namespace
      type: string
      default: ""
    - name: environment
      type: string
    - name: cluster
      type: string
      description: "the name of the cluster to deploy the thing. basically use for one-off jobs"

    mixins:
      check_version: &check_version |
        if ! kubectl version >/dev/null; then
          echo '`kodedeploy register` requires an valid k8s api access`' 1>&2
          exit 1
        fi

      get_parames: &get_parames |
        ns={{ get "namespace" }}
        env={{ get "environment" }}
        cluster={{ get "cluster" }}

        ns=${ns?missing value. specify k8s namespace = your team, product, or project name}
        env=${env?missing value. specify e.g. production, staging, test, preview}
        cluster=${cluster?missing value. specify cluster}

        # this equals to the iam user name
        instance_name="${env}-${ns}-${cluster}"

      make_etc_codedeploy_agent_conf: &make_etc_codedeploy_agent_conf |
        etc_codedeploy_agent_conf_dir=.awscodectl/tmp/${instance_name}/etc-codedeploy-agent-conf
        mkdir -p "${etc_codedeploy_agent_conf_dir}"

        instance=`aws deploy get-on-premises-instance --instance-name ${instance_name} 2> /dev/null`
        if [ "${instance}" == "" ]; then
          if [ $dryrun == "true" ]; then
            echo "aws deploy \"${instance_name}\" registered (dry run)"
            touch codedeploy.onpremises.yml "${etc_codedeploy_agent_conf_dir}/codedeploy.onpremises.yml"
            echo "aws iam put-user-policy codedeploy-namespace to ${instance_name} (dry run)"
          else
            # include credentials in stdout, so redirect to /dev/null
            aws deploy register \
                --instance-name ${instance_name} \
                --tags Key=kodedeployenv,Value={{ get "environment" }}\
                       Key=kodedeploycluster,Value={{ get "cluster" }}\
                       Key=kodedeployns,Value={{ get "namespace" }} > /dev/null
            mv codedeploy.onpremises.yml "${etc_codedeploy_agent_conf_dir}/"
            aws iam put-user-policy \
              --user-name ${instance_name} \
              --policy-name codedeploy-namespace \
              --policy-document file://codedeploy-namespace.iampolicy.json
          fi
        fi

      make_opt_codedeploy_agent_conf: &make_opt_codedeploy_agent_conf |
        opt_codedeploy_agent_conf_dir=.awscodectl/tmp/${instance_name}/opt-codedeploy-agent-conf
        mkdir -p "${opt_codedeploy_agent_conf_dir}"

        cat <<EOS > "${opt_codedeploy_agent_conf_dir}/codedeployagent.yml"
        ---
        :log_aws_wire: false
        :log_dir: '/var/log/aws/codedeploy-agent/'
        :pid_dir: '/opt/codedeploy-agent/state/.pid/'
        :program_name: codedeploy-agent
        :root_dir: '/opt/codedeploy-agent/deployment-root'
        :verbose: false
        :wait_between_runs: 1
        :proxy_uri:
        :max_revisions: 5
        EOS

      make_cloudwatch_agent_conf: &make_cloudwatch_agent_conf |
        keyid=$(cat ${etc_codedeploy_agent_conf_dir}/codedeploy.onpremises.yml | grep aws_access_key_id | cut -f2 -d' ')
        secretkey=$(cat ${etc_codedeploy_agent_conf_dir}/codedeploy.onpremises.yml | grep aws_secret_access_key | cut -f2 -d' ')

        aws_dir=.awscodectl/tmp/${instance_name}/cloudwatch-agent-aws
        mkdir -p "${aws_dir}"

        cat <<EOS > ${aws_dir}/credentials
        [AmazonCloudWatchAgent]
        aws_access_key_id = $keyid
        aws_secret_access_key = $secretkey
        region=ap-northeast-1

        [awscloudwatchagent]
        aws_access_key_id = $keyid
        aws_secret_access_key = $secretkey
        region=ap-northeast-1
        EOS

        cloudwatch_agent_conf_dir=.awscodectl/tmp/${instance_name}/cloudwatch-agent-conf
        mkdir -p "${cloudwatch_agent_conf_dir}"

        # See https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html
        cat <<EOS > ${cloudwatch_agent_conf_dir}/config.json
        {
          "logs": {
            "logs_collected": {
              "files": {
                "collect_list": [
                  {
                    "file_path": "/var/log/messages",
                    "log_group_name": "messages"
                  },
                  {
                    "file_path": "/opt/codedeploy-agent/deployment-root/**/scripts.log",
                    "log_group_name": "kodedeploy/deployments",
                    "log_stream_name": "scripts"
                  }
                ]
              }
            }
          }
        }
        EOS

      apply: &apply |
        kubectl --dry-run=$dryrun create namespace kodedeploy --dry-run=$dryrun

        kubectl --dry-run=$dryrun --namespace kodedeploy \
          create secret generic --from-file=${etc_codedeploy_agent_conf_dir} $(basename $etc_codedeploy_agent_conf_dir)

        kubectl --dry-run=$dryrun --namespace kodedeploy \
          create configmap --from-file="${opt_codedeploy_agent_conf_dir}" $(basename $opt_codedeploy_agent_conf_dir)

        kubectl --dry-run=$dryrun --namespace kodedeploy \
          create secret generic --from-file="${aws_dir}" $(basename $aws_dir)

        kubectl --dry-run=$dryrun --namespace kodedeploy \
          create configmap --from-file="${cloudwatch_agent_conf_dir}" $(basename $cloudwatch_agent_conf_dir)

        kubectl --dry-run=$dryrun --namespace kodedeploy apply -f serviceaccount.yaml

        kubectl --dry-run=$dryrun --namespace kodedeploy apply -f deploy.yaml

        kubectl get secret,configmap,pod -n kodedeploy

    script:
    - |
      dryrun=false
    - *check_version
    - *get_parames
    - *make_etc_codedeploy_agent_conf
    - *make_opt_codedeploy_agent_conf
    - *make_cloudwatch_agent_conf
    - *apply

    tasks:
      plan:
        script:
        - |
          dryrun=true
        - *check_version
        - *get_parames
        - *make_etc_codedeploy_agent_conf
        - *make_opt_codedeploy_agent_conf
        - *make_cloudwatch_agent_conf
        - *apply

      rollback:
        mixins:
          deregister: &deregister |
            # Avoid the `An error occurred (DeleteConflict) when calling the DeleteUser operation: Cannot delete entity, must delete policies first.` error
            aws iam list-user-policies --user-name "${instance_name}"
            aws iam delete-user-policy --user-name "${instance_name}" --policy-name codedeploy-agent
            aws iam delete-user-policy --user-name "${instance_name}" --policy-name codedeploy-namespace

            # Avoid the `An error occurred (DeleteConflict) when calling the DeleteUser operation: Cannot delete entity, must delete access keys first.` error
            key_id=$(aws iam list-access-keys --user-name "${instance_name}" | jq -r '.AccessKeyMetadata[].AccessKeyId')
            aws iam delete-access-key --user-name "${instance_name}" --access-key-id "${key_id}"

            aws deploy deregister-on-premises-instance --instance-name "${instance_name}"
            aws iam delete-user --user-name "${instance_name}"
          apply_rollback: &apply_rollback |
            kubectl --namespace kodedeploy get secret,configmap,pod

            kubectl --namespace kodedeploy delete -f deploy.yaml

            kubectl --namespace kodedeploy delete -f serviceaccount.yaml

            kubectl --namespace kodedeploy delete configmap cloudwatch-agent-conf

            kubectl --namespace kodedeploy delete configmap opt-codedeploy-agent-conf

            kubectl --namespace kodedeploy delete secrets cloudwatch-agent-aws

            kubectl --namespace kodedeploy delete secrets etc-codedeploy-agent-conf

            kubectl --namespace kodedeploy get secret,configmap,pod
        script:
          - *check_version
          - *get_parames
          - *deregister
          - *apply_rollback

  run:
    parameters:
    - name: namespace
      type: string
      default: ""
    - name: environment
      type: string
    - name: command
      type: string
    - name: application
      default: ""
    - name: directory
      default: ""
    - name: bucket
      type: string
    - name: image
      type: string
      default: "quay.io/roboll/helmfile:v0.40.1"
    script: |
      clusters=$(get all clusters)

      for c in $clusters; do
        echo cluster: $c
        cluster=$c
        break
      done

      ./awscodectl deploy \
        --namespace {{ get "namespace" }} \
        --environment {{ get "environment" }} \
        --application "{{ get "application" }}" \
        --directory "{{ get "directory" }}" \
        --bucket "{{ get "bucket" }}" \
        --image "{{ get "image" }}" \
        --cluster "$cluster"

  deploy:
    parameters:
    - name: namespace
      type: string
      default: ""
    - name: environment
      type: string
    - name: command
      type: string
    - name: application
      default: ""
    - name: directory
      default: ""
    - name: bucket
      type: string
    - name: image
      type: string
      default: "quay.io/roboll/helmfile:v0.40.1"
    - name: cluster
      type: string
      description: "the name of the cluster to deploy the thing. basically use for one-off jobs"
      default: ""
    - name: revision
      type: string
      default: myrev

    mixins:
      get_parames: &get_parames |
        ns={{ get "namespace" }}
        env={{ get "environment" }}
        app={{ get "application" }}
        source={{ get "directory" }}
        bucket={{ get "bucket" }}
        cluster={{ get "cluster" }}
        revision={{ get "revision" }}

        if [ "${ns}" == "" -a "${app}" == "" ]; then
          dir=$(basename $(pwd))
          app="${dir}"
          ns="${dir}"
        elif [ "${app}" != "" -a "${ns}" == "" ]; then
          echo "application is specified but namespace. they must be specified at once, or completely ommitted." 1>&2
          exit 1
        elif [ "${app}" == "" -a "${ns}" != "" ]; then
          echo "namespace is specified but application. they must be specified at once, or completely omitted." 1>&2
          exit 1
        fi

        if [ "${source}" == "" ]; then
          source="$(pwd)"
        fi

        env=${env?missing value. specify e.g. production, staging, test, preview}
        bucket=${bucket?required value}

        bundletype=zip
        key=codedeploy/${app}/${revision}.${bundletype}

        app="kodedeploy-env-${env}-ns-${ns}-app-${app}"

        # ${clusterenv}_${clusterns} e.g. production-myproduct, staging-myteam
        # Note that one or more groups with the same name are created per application,
        # as groups are isolated by apps
        if [ "${cluster}" == "" ]; then
          group="kodedeploy-env-${env}-ns-${ns}"
        else
          group="kodedeploy-env-${env}-ns-${ns}-cluster-${cluster}"
        fi

      make_cocedeploy_conf: &make_cocedeploy_conf |
        mkdir -p "${source}"
        cd "${source}"

        before_install=before-install.sh
        after_install=after-install.sh

        cat <<EOS > appspec.yml
        version: 0.0
        os: linux
        #files:
        #  - source: deploy
        #    destination: /deploy
        hooks:
          BeforeInstall:
          - location: ${before_install}
            timeout: 180
          AfterInstall:
          - location: ${after_install}
            timeout: 180
        EOS

        cat <<EOS > "${before_install}"
        #!/usr/bin/env bash
        #rm -rf /deploy
        echo before install
        EOS

        cat <<EOS > "after-install-lib.sh"
        #!/usr/bin/env bash
        set -eo pipefail
        error() {
          status=(${PIPESTATUS[@]})
          echo status=${status[@]} 1>&2
          for ((i=0; i<${#status[@]}; i++)); do
            c=${status[$i]}
            echo exit status of command $i: $c
            if [ "$c" != 0 ]; then
              return $c
            fi
          done
        }
        trap error ERR
        EOS

        cat <<'EOS' > "${after_install}"
        #!/usr/bin/env bash
        set -vx
        wd="${WORK_DIR:-/opt/codedeploy-agent/deployment-root/${DEPLOYMENT_GROUP_ID}/${DEPLOYMENT_ID}/deployment-archive}"
        image="{{ get "image" }}"
        # To use kodedeploy pod's serviceaccount to access Kubernetes API
        sd="/var/run/secrets/kubernetes.io/serviceaccount/"
        cmd="{{.command}}"

        log_group=kodedeploy/deploys/${DEPLOYMENT_GROUP_NAME}

        exec 2> >(sed "s/^/err/" | cloudwatch-logger -t "${log_group}" ${DEPLOYMENT_ID}/stderr)

        docker run -v "${wd}:${wd}" -v "${sd}:${sd}" \
        -e "KUBERNETES_SERVICE_HOST=${KUBERNETES_SERVICE_HOST}" \
        -e "KUBERNETES_SERVICE_PORT=${KUBERNETES_SERVICE_PORT}" \
        -e "KUBE_DNS_SERVICE_HOST=${KUBE_DNS_SERVICE_HOST}" \
        -e "KUBE_DNS_SERVICE_PORT=${KUBE_DNS_SERVICE_PORT}" \
        --rm -w "${wd}" \
        "${image}" bash -c ". ./after-install-lib.sh; ${cmd}" | cloudwatch-logger -t "${log_group}" ${DEPLOYMENT_ID/stdout}
        exit ${PIPESTATUS[0]}
        EOS
        chmod +x ${before_install} ${after_install}

        cd -
        tree "${source}"

      create_codedeploy_application: &create_codedeploy_application |
        aws deploy get-application --application-name $app 2> /dev/null
        code=$?
        if [ $code -eq 255 ]; then
          if [ $dryrun == "true" ]; then
            echo "aws deploy created application \"$app\" (dry run)"
          else
            if ! aws deploy create-application --application-name $app >/dev/null; then
              echo failed creating $app 1>&2
              exit 1
            fi
          fi
        fi

      push_codedeploy_application: &push_codedeploy_application |
        if [ $dryrun == "true" ]; then
          echo "aws deploy pushed ${source} to s3://${bucket}/${key} (dry run)"
        else
          aws deploy push \
            --application-name ${app} \
            --description "${myrev}" \
            --ignore-hidden-files \
            --s3-location s3://${bucket}/${key} \
            --source ${source}
        fi

      get_or_create_service_role: &get_or_create_service_role |
        # TODO better name
        role=kodedeploy
        aws iam get-role --role-name "${role}" >.kodedeploy.role.json 2> /dev/null
        code=$?
        if [ $code -eq 255 ]; then
          if [ $dryrun == "true" ]; then
            echo "aws iam created role \"${role}\" (dry run)"
            service_role_arn="dummy"
            touch.kodedeploy.role.json
          else
            aws iam create-role --role-name "${role}" --assume-role-policy-document '{
            "Version": "2012-10-17",
            "Statement": [
              {
                "Sid": "",
                "Effect": "Allow",
                "Principal": {
                  "Service": "codedeploy.amazonaws.com"
                },
                "Action": "sts:AssumeRole"
              }
            ]
            }' >.kodedeploy.role.json
          fi
        fi
        service_role_arn="$(jq -r .Role.Arn .kodedeploy.role.json)"
        rm .kodedeploy.role.json

      create_deployment_group: &create_deployment_group |
        aws deploy get-deployment-group --application-name "${app}" \
          --deployment-group-name "${group}" 2>/dev/null
        code=$?
        if [ $code -eq 255 ]; then
          if [ "${cluster}" == "" ]; then
            tagset='{
              "onPremisesTagSetList":[
                [{"Key":"kodedeployenv","Value":"'${env}'","Type":"KEY_AND_VALUE"}],
                [{"Key":"kodedeployns","Value":"'${ns}'","Type":"KEY_AND_VALUE"}]
              ]
            }'
          else
            tagset='{
              "onPremisesTagSetList":[
                [{"Key":"kodedeployenv","Value":"'${env}'","Type":"KEY_AND_VALUE"}],
                [{"Key":"kodedeploycluster","Value":"'${cluster}'","Type":"KEY_AND_VALUE"}],
                [{"Key":"kodedeployns","Value":"'${ns}'","Type":"KEY_AND_VALUE"}]
              ]
            }'
          fi
          if [ $dryrun == "true" ]; then
            echo "aws deploy created deployment-group \"${group}\" (dry run)"
            echo on-premisses-tag-set: ${tagset}
          else
            aws deploy create-deployment-group \
              --application-name $app \
              --deployment-group-name "${group}" \
              --service-role-arn "${service_role_arn}" \
              --on-premises-tag-set "${tagset}" \
              >/dev/null
            if [ $? -ne 0 ]; then
              echo "failed creating \"${group}\"" 1>&2
              exit 1
            fi
          fi
        fi

      deploy: &deploy |
        if [ $dryrun == "true" ]; then
          echo "aws deploy created deployment for \"$app\" (dry run)"
        else
          config=CodeDeployDefault.AllAtOnce
          aws deploy create-deployment \
            --application-name ${app} \
            --deployment-config-name ${config} \
            --deployment-group-name ${group} \
            --s3-location bucket=${bucket},key=${key},bundleType=${bundletype} > .kodedeploy.deployment.json

          deploy_id="$(jq -r .deploymentId .kodedeploy.deployment.json)"
          echo "aws deploy created deployment for \"$app\""

          aws deploy wait deployment-successful --deployment-id "${deploy_id}"
          code=$?
          if [ $code -ne 0 ]; then
            aws logs get-log-events \
              --log-group-name kodedeploy/deploys/${group} \
              --log-stream-name ${deploy_id}/stderr | jq -r '.events[].message' | sed '/^\s*$/d'

            echo exit code: $code 1>&2

            aws deploy get-deployment --deployment-id "${deploy_id}" > .kodedeploy.deployment.json
            err_code=$(jq -r .deploymentInfo.errorInformation.code .kodedeploy.deployment.json)
            err_mesg=$(jq -r .deploymentInfo.errorInformation.message .kodedeploy.deployment.json)
            echo "${err_code}: ${err_mesg}" 1>&2
            exit $code
          fi
          aws logs get-log-events \
            --log-group-name kodedeploy/deploys/${group} \
            --log-stream-name ${deploy_id} | jq -r '.events[].message' | sed '/^\s*$/d'
        fi

    script:
    - |
      dryrun=false
    - *get_parames
    - *make_cocedeploy_conf
    - *create_codedeploy_application
    - *push_codedeploy_application
    - *get_or_create_service_role
    - *create_deployment_group
    - *deploy

    tasks:
      plan:
        script:
        - |
          dryrun=true
        - *get_parames
        - *make_cocedeploy_conf
        - *create_codedeploy_application
        - *push_codedeploy_application
        - *get_or_create_service_role
        - *create_deployment_group
        - *deploy
